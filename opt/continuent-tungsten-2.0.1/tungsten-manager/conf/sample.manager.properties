# Default properties file for Service Manager.  Copy this file to 
# manager.properties and update values as needed for your installation. 
#
# Note:  The ${manager.home} property is set automatically by Service 
# Manager start scripts.  As long as you are running the service manager
# from these scripts no changes are necessary.  
#


# Persistent storage properties. 
manager.db.url=jdbc:derby:${manager.home}/private/db;create=true
manager.db.driver=org.apache.derby.jdbc.EmbeddedDriver

# Location of hedera.properties file. 
manager.gc.hedera_properties=${manager.home}/conf/hedera.properties

# Location of properties files for replicator and router
manager.replicator.properties=${manager.home}/../tungsten-replicator/conf/replicator.properties
manager.router.properties=${manager.home}/../tungsten-sqlrouter/conf/router.properties

# If default_join=true and manager was not in a group before, it will try to
# join the given GC. 
manager.gc.default_join=true
manager.gc.group=default
manager.gc.member=${manager.hostname}

manager.autoOnline=true

# RMI host and default port. 
manager.rmi.host=localhost
manager.rmi.port=9997
# Port number for file server
manager.file_server.port=9998

# Service directory containing property files for handlers. 
manager.handlers.dir=${manager.home}/handlers

# A list of resources that should be activated automatically on initial
# start-up. Effective only if manager has joined GC (eg. default_join=true).
manager.handlers.auto_activate=ClusterManagementHandler

# The factory class used to handle event routing, state management, and membership
# management
manager.factory=com.continuent.tungsten.manager.core.mapping.MappingFactory

#
# Class that will receive notifications from the monitor and will take appropriate actions
# via the manager, with other components - for failover, etc.
#
manager.cluster.policy=com.continuent.tungsten.manager.cluster.policy.SimplePolicyManager
#
# Default mode for the policy manager
#
manager.policy.mode=automatic

# 
# JMX proxy for managing replication.
#
manager.replicator.proxy=com.continuent.tungsten.manager.resource.proxy.ReplicatorManagerProxyImplV1

#
# Indicates whether to start up the integral monitor
#
manager.monitor.start=false

#
# Indicates whether to start up the policy manager.
# This should only be set to false if problems are
# suspected with the policy manager.
#
manager.policy.start=true

#
# Indicates which address can be used to
# arbitrate determination of a primary partition.
#
manager.policy.arbitrator=www.google.com,www.bloomberg.com
 
#
# Maximum number of seconds to wait for router answers
# After this delay, a silent router will be discarded
# Negative or zero values means "wait forever"
#
manager.idle.router.timeout=60

#
# Timeout for internal synchronization requests. 
#
manager.sync.timeout=60000

#
# Indicates how often to sample the policy manager
# in order to determine it's liveness. Default
# is to sample every 1 second
#
policy.liveness.samplePeriodSecs=2

#
# Indicates how many consecutive sample periods
# can pass without progress before it becomes
# a liveness issue. The default is 30 sample periods
# which, when combined with the default sample period,
# equates to 30 seconds
#
policy.liveness.thresholdPeriods=30

#
# Indicates how long, in seconds, dataSourcePing() should
# use for a timeout in order to establish liveness
# for a data source
#
policy.liveness.dsPingTimeout=5

#
# Indicates the timeout length, in seconds, a ping operation
# uses when trying to establish liveness of a host. 
#
policy.liveness.hostPingTimeout=5

#
# Comma-separated list of ping method(s) to use when checking for 
# host liveness.  Supported method names are default (uses 
# TCP/IP port 7 for non-root account, otherwise ICMP), and ping
# (uses OS ping utility). 
#
policy.liveness.hostPingMethods=default,ping

#
# If this property is set to true, the enterprise rules
# will set an online slave datasource to OFFLINE if the 
# associated replicator is not ONLINE. The default
# is to fence the replicator to OFFLINE.
#
policy.fence.slaveReplicator=false

#
# If this property is set to true, the enterprise rules
# will set an online master datasource to OFFLINE if the 
# associated replicator is not ONLINE. The default
# is to leave master datasources ONLINE independent of the
# replicator state.
# 
policy.fence.masterReplicator=false

#
# Automatically determine optimal level for router notification
# timeouts.
#
policy.notification.adjust.auto=false

#
# Interval to use in order to backoff router notification timeout
# after it's previously adjusted to a higher value. This
# defaults to an hour
#
policy.notification.adjust.backoff=2000

#
# The maximum possible timeout for notifications.  After this number,
# warnings are generated in the log.
#
policy.notification.max.timeout=5000

#
# The number of notification delivery failures that will
# trigger an adjustment.
#
policy.notification.adjust.threshold=5

#
# The number of successful notification deliveries that
# will trigger a backoff
#
policy.notification.adjust.success.threshold=10

#
# The number of backoff attempts before the timeout
# will be leveled.
#
policy.notification.adjust.backoff.try=3


#
# Indicates that the manager should release
# a local VIP the second time it is started by the wrapper.
# In this case, the first time will just be a normal manager startup but
# subsequent restarts will increment the number of invocations.
#
manager.vip.release.invocation=2

#
# Path to executable VIP release script
#
manager.vip.release.script=${manager.home}/../cluster-home/bin/vip-release-helper

#
# Indicates that the manager should invoke the local fail-safe
# script during the fifth invocation of the manager. The manager
# will then exit
#
manager.failsafe.invocation=5

#
# Path to executable local fail-safe script
#
manager.failsafe.script=${manager.home}/../cluster-home/bin/fail-safe-helper true

#
# Number of dispatch periods after which a manager failure causes associated
# resources to be marked as failed as well. 
#
manager.fail.threshold=0

#
# Number of seconds per fail dispatch
#
manager.fail.dispatchPeriod=10

#
# Indicates the complete set of cluster members
#
manager.global.members=db1,db2,db3
#
# Indicates the a set of hosts that can be used to test
# for cluster interconnect network connectivity.  These
# hosts must be accessible either on the same network
# used by the members or must route throught the same
# network.
#
manager.global.witnesses=db10,db11

#
# The following properties are used by VIP management
#

#
# Indicates whether or not the manager should maintain a VIP
# that is bound to the master database server.
#
vip.isEnabled=false


#
# Indicates the IP address to be bound for the VIP
#
vip.address=192.168.0.155

#
# Indicates the full path to the ifconfig command
# for the user that runs tungsten
#
vip.ifconfig_path=/sbin/ifconfig

#
# Indicates the full path to the arp command
# for the user that runs tungsten
#
vip.arp_path=/usr/sbin/arp

#
# Indicates the full path to the script that we use to
# encapsulate bind/release/delete of VIP
#
vip.vip_management_helper=${manager.home}/bin/vip_management_helper

#
# Indicates the prefix to use when executing root-level
# commands
#
manager.sudoCommandPrefix=sudo

#
# Indicates that the manager should keep slaves
# in a read-only state at all times and also
# ensure that masters are read/write
#
manager.readOnlySlaves=true

#
# Indicates that the manager should forward replicator notifications
# to routers.  This is turned on by default
#
manager.notifications.send=true

#
# Indicates how long to wait for a router notification to be delivered
# This quantity is in milliseconds
#
manager.notifications.timeout=100

#
# The amount of time, in milliseconds, to wait for routers to respond
# when executing interactive status commands in cctrl.
#
manager.router.timeout=500

#
# The number of seconds to which a slave must be current with the master
# in order to qualify as a candidate for failover.  Defaults to
# 15 minutes (900 seconds).
#
policy.slave.promotion.latency.threshold=900


#
# Indicates whether or not to manage the VIP in a failsafe way.
# If this option is set to true, the VIP, if enabled, will
# be released if a manager is not in maintenance mode and
# is the manager for the master data source.  Setting
# it to false leaves the VIP bound if a manager exits.
#
manager.vip.isFailSafe=true

#
# There are some corner cases where it is not sufficient to simply 
# close existing connections, on the connectivity side of things, in order
# to have any outstanding transactions aborted. In many cases, particularly if
# there's a large transaction pending, the transaction may even complete after
# the connection that started it goes away.  So the replicator now has
# a way of killing all connections that are not the connection currently in 
# use by Tungsten replicator, and this is done by looking at the username
# for each MySQL connection.  So it's also critical that applications don't
# also use the 'tungsten' username or those connections may inadvertently
# remain active. 
# 
manager.destructive.purge=true


